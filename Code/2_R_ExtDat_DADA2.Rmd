---
title: "16S RNA sequencing analysis Tutorial"
---


***Load packages***
```{r}
#BiocManager::install("dada2") #mm - to install the software
library(dada2); packageVersion("dada2")
```

***Set path to fastq files***

```{r}
path <- "~/Desktop/fastQ" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```

Now we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.

```{r}
fnFs <- sort(list.files(path, pattern="_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_R1.fastq.gz"), `[`, 1)
```

***Inspect read quality profiles***

We start by visualizing the quality profiles of the forward reads:
```{r}
plotQualityProfile(fnFs[1:2])
```
***Filter and trim***
Assign the filenames for the filtered fastq.gz files.

```{r}
# Place filtered files in filtered/ subdirectory
filtFs.new <- file.path(path, "filtered.new", paste0(sample.names, "_F_filt.new.fastq.gz"))
filtRs.new <- file.path(path, "filtered.new", paste0(sample.names, "_R_filt.new.fastq.gz"))

names(filtFs.new) <- sample.names
```

We’ll use standard filtering parameters: 
maxN=0 :Maximum number of ambiguous bases to allow within the a read. for dada2, it requires maxN=0
truncLen = the maximum length, after which the bases should be cut off.
truncQ=2  :Truncate reads at the first instance of a quality score less than or equal to truncQ.
rm.phix=TRUE :If TRUE, discard reads that match against the phiX genome, as determined by isPhiX. 
maxEE=2 :The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.

```{r}
out.new <- filterAndTrim(fnFs, filtFs.new, fnRs, filtRs.new, truncLen=c(250,250), maxN=c(0,0), maxEE=c(2,2), truncQ=c(2,2), rm.phix=TRUE,
                     compress=TRUE, multithread=F) # On Windows set multithread=FALSE
head(out.new)

```

***Learn the Error Rates***

```{r}
errF.new <- learnErrors(filtFs.new, multithread=F) # On Windows set multithread=FALSE
```

```{r}
errR.new <- learnErrors(filtRs.new, multithread=F) # On Windows set multithread=FALSE
```



It is always worthwhile, as a sanity check if nothing else, to visualize the estimated error rates:
```{r}
plotErrors(errF.new, nominalQ=TRUE)
```
***Sample Inference***
We are now ready to apply the core sample inference algorithm to the filtered and trimmed sequence data.
```{r}
dadaFs.new <- dada(filtFs.new, errF.new, multithread=F)
dadaRs.new <- dada(filtRs.new, errR.new, multithread=F)
```

Inspecting the returned dada-class object:
```{r}
dadaFs.new[[1]]
dadaRs.new[[1]]
```

***Merge paired reads***
```{r}
mergers.new <- mergePairs(dadaFs.new, filtFs.new, dadaRs.new, filtRs.new, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers.new[[1]])
```
***Construct sequence table***
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.
```{r}
seqtab.new <- makeSequenceTable(mergers.new)
dim(seqtab.new)
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab.new)))
```

***Remove chimeras***
```{r}
seqtab.nochim.new <- removeBimeraDenovo(seqtab.new, method="consensus", multithread=F, verbose=TRUE)
dim(seqtab.nochim.new)
```

```{r}
sum(seqtab.nochim.new)/sum(seqtab.new)
```

***Track reads through the pipeline***
As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out.new, sapply(dadaFs.new, getN), sapply(dadaRs.new, getN), sapply(mergers.new, getN), rowSums(seqtab.nochim.new))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

***Assign taxonomy***

```{r}
#RC = TRUE
taxa.new <- assignTaxonomy(seqs = seqtab.nochim.new, refFasta = "~/Dropbox/BEECH_16s_Analysis/Data/Tax/silva_nr99_v138.1_train_set.fa.gz", tryRC = T, multithread=F)
```

************* mm -- If you used the 'silva_nr99_v138.1_wSpecies_train_set.fa.gz' reference file, this step is not necessary.
```{r}
#RC = TRUE
taxa.new2 <- addSpecies(taxtab = taxa.new, refFasta =  "~/Dropbox/BEECH_16s_Analysis/Data/Tax/silva_species_assignment_v138.1.fa.gz", tryRC = T)

```

Let’s inspect the taxonomic assignments:
```{r}
taxa.print <- taxa.new2 # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

***Save your data at this point***
```{r}
seqtab.nochim.bang <- seqtab.nochim.new
taxa.bang <- taxa.new2
save(seqtab.nochim.bang, file="~/Dropbox/BEECH_16s_Analysis/Data/ExtDat_Bang_CountTab.RData") #Count table
save(taxa.bang, file="~/Dropbox/BEECH_16s_Analysis/Data/EXtDat_Bang_TaxTab.RData") #Taxonomy table
```


Paired reads - SEEM
***Set path to fastq files***

```{r}
path <- "~/Desktop/fastQ/SEEM" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```

Now we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.

```{r}
fnFs <- sort(list.files(path, pattern="_1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_2.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_1.fastq.gz"), `[`, 1)
```

***Inspect read quality profiles***

We start by visualizing the quality profiles of the forward reads:
```{r}
plotQualityProfile(fnFs[1:2])
```
***Filter and trim***
Assign the filenames for the filtered fastq.gz files.

```{r}
# Place filtered files in filtered/ subdirectory
filtFs.new <- file.path(path, "filtered.new", paste0(sample.names, "_F_filt.new.fastq.gz"))
filtRs.new <- file.path(path, "filtered.new", paste0(sample.names, "_R_filt.new.fastq.gz"))

names(filtFs.new) <- sample.names
```

We’ll use standard filtering parameters: 
maxN=0 :Maximum number of ambiguous bases to allow within the a read. for dada2, it requires maxN=0
truncLen = the maximum length, after which the bases should be cut off.
truncQ=2  :Truncate reads at the first instance of a quality score less than or equal to truncQ.
rm.phix=TRUE :If TRUE, discard reads that match against the phiX genome, as determined by isPhiX. 
maxEE=2 :The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.

```{r}
out.new <- filterAndTrim(fnFs, filtFs.new, fnRs, filtRs.new, truncLen=c(250,250), maxN=c(0,0), maxEE=c(2,2), truncQ=c(2,2), rm.phix=TRUE,
                     compress=TRUE, multithread=F) # On Windows set multithread=FALSE
head(out.new)

```

***Learn the Error Rates***

```{r}
errF.new <- learnErrors(filtFs.new, multithread=F) # On Windows set multithread=FALSE
```

```{r}
errR.new <- learnErrors(filtRs.new, multithread=F) # On Windows set multithread=FALSE
```



It is always worthwhile, as a sanity check if nothing else, to visualize the estimated error rates:
```{r}
plotErrors(errF.new, nominalQ=TRUE)
```
***Sample Inference***
We are now ready to apply the core sample inference algorithm to the filtered and trimmed sequence data.
```{r}
dadaFs.new <- dada(filtFs.new, errF.new, multithread=F)
dadaRs.new <- dada(filtRs.new, errR.new, multithread=F)
```

Inspecting the returned dada-class object:
```{r}
dadaFs.new[[1]]
dadaRs.new[[1]]
```

***Merge paired reads***
```{r}
mergers.new <- mergePairs(dadaFs.new, filtFs.new, dadaRs.new, filtRs.new, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers.new[[1]])
```
***Construct sequence table***
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.
```{r}
seqtab.new <- makeSequenceTable(mergers.new)
dim(seqtab.new)
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab.new)))
```

***Remove chimeras***
```{r}
seqtab.nochim.new <- removeBimeraDenovo(seqtab.new, method="consensus", multithread=F, verbose=TRUE)
dim(seqtab.nochim.new)
```

```{r}
sum(seqtab.nochim.new)/sum(seqtab.new)
```

***Track reads through the pipeline***
As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out.new, sapply(dadaFs.new, getN), sapply(dadaRs.new, getN), sapply(mergers.new, getN), rowSums(seqtab.nochim.new))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

***Assign taxonomy***

```{r}
#RC = TRUE
taxa.new <- assignTaxonomy(seqs = seqtab.nochim.new, refFasta = "~/Dropbox/BEECH_16s_Analysis/Data/Tax/silva_nr99_v138.1_train_set.fa.gz", tryRC = T, multithread=F)
```

************* mm -- If you used the 'silva_nr99_v138.1_wSpecies_train_set.fa.gz' reference file, this step is not necessary.
```{r}
#RC = TRUE
taxa.new2 <- addSpecies(taxtab = taxa.new, refFasta =  "~/Dropbox/BEECH_16s_Analysis/Data/Tax/silva_species_assignment_v138.1.fa.gz", tryRC = T)

```

Let’s inspect the taxonomic assignments:
```{r}
taxa.print <- taxa.new2 # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

***Save your data at this point***
```{r}
seqtab.nochim.seem <- seqtab.nochim.new
taxa.seem <- taxa.new2
save(seqtab.nochim.seem, file="~/Dropbox/BEECH_16s_Analysis/Data/ExtDat_SEEM_CountTab.RData") #Count table
save(taxa.seem, file="~/Dropbox/BEECH_16s_Analysis/Data/EXtDat_SEEM_TaxTab.RData") #Taxonomy table
```



Single Reads - Afribiota

***Load packages***
```{r}
#BiocManager::install("dada2") #mm - to install the software
library(dada2); packageVersion("dada2")
```

***Set path to fastq files***

```{r}
path <- "~/Desktop/fastQ" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```

Now we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.

```{r}
fnFs <- sort(list.files(path, pattern=".fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), ".fastq.gz"), `[`, 1)
```

***Inspect read quality profiles***

We start by visualizing the quality profiles of the forward reads:
```{r}
plotQualityProfile(fnFs[1:2])
```
***Filter and trim***
Assign the filenames for the filtered fastq.gz files.

```{r}
# Place filtered files in filtered/ subdirectory
filtFs.new <- file.path(path, "filtered.new", paste0(sample.names, "_filt.new.fastq.gz"))

names(filtFs.new) <- sample.names
```

We’ll use standard filtering parameters: 
maxN=0 :Maximum number of ambiguous bases to allow within the a read. for dada2, it requires maxN=0
truncLen = the maximum length, after which the bases should be cut off.
truncQ=2  :Truncate reads at the first instance of a quality score less than or equal to truncQ.
rm.phix=TRUE :If TRUE, discard reads that match against the phiX genome, as determined by isPhiX. 
maxEE=2 :The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.

```{r}
out.new <- filterAndTrim(fnFs, filtFs.new, truncLen=c(250), maxN=c(0), maxEE=c(2), truncQ=c(2), rm.phix=TRUE, compress=TRUE, multithread=F) # On Windows set multithread=FALSE
head(out.new)

```

***Learn the Error Rates***

The following runs in about 3 minutes on a 2013 Macbook Pro:
```{r}
errF.new <- learnErrors(filtFs.new, multithread=F) # On Windows set multithread=FALSE
```

It is always worthwhile, as a sanity check if nothing else, to visualize the estimated error rates:
```{r}
plotErrors(errF.new, nominalQ=TRUE)
```

***Sample Inference***
We are now ready to apply the core sample inference algorithm to the filtered and trimmed sequence data.
```{r}
dadaFs.new <- dada(filtFs.new, errF.new, multithread=F)
```

Inspecting the returned dada-class object:
```{r}
dadaFs.new[[1]]
```

***Construct sequence table***
```{r}
seqtab.new <- makeSequenceTable(dadaFs.new)
dim(seqtab.new)
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab.new)))
```

***Remove chimeras***
```{r}
seqtab.nochim.new <- removeBimeraDenovo(seqtab.new, method="consensus", multithread=F, verbose=TRUE)
dim(seqtab.nochim.new)
```

```{r}
sum(seqtab.nochim.new)/sum(seqtab.new)
```

***Track reads through the pipeline***
As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out.new, sapply(dadaFs.new, getN), sapply(dadaRs.new, getN), sapply(mergers.new, getN), rowSums(seqtab.nochim.new))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

***Assign taxonomy***
```{r}
#RC = TRUE
taxa.new <- assignTaxonomy(seqs = seqtab.nochim.new, refFasta = "~/Dropbox/BEECH_16s_Analysis/Data/Tax/silva_nr99_v138.1_train_set.fa.gz", tryRC = T, multithread=F)
```


************* mm -- If you used the 'silva_nr99_v138.1_wSpecies_train_set.fa.gz' reference file, this step is not necessary.
```{r}
#RC = TRUE
taxa.new2 <- addSpecies(taxtab = taxa.new, refFasta =  "~/Dropbox/BEECH_16s_Analysis/Data/Tax/silva_species_assignment_v138.1.fa.gz", tryRC = T)

```

Let’s inspect the taxonomic assignments:
```{r}
taxa.print <- taxa.new2 # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

***Save your data at this point***
```{r}
seqtab.nochim.afr <- seqtab.nochim.new
taxa.afr <- taxa.new2
save(seqtab.nochim.afr, file="~/Dropbox/BEECH_16s_Analysis/Data/ExtDat_Afr_CountTab.RData") #Count table
save(taxa.afr, file="~/Dropbox/BEECH_16s_Analysis/Data/EXtDat_Afr_TaxTab.RData") #Taxonomy table
```

